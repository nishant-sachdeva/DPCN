# -*- coding: utf-8 -*-
"""MPROC_DPCN_PROJECT_PART1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fFWimNIb1JfG344XjnqlM8N-m2zfwFY-

Here are the steps we are looking to implement.

1. Get the Math overflow data, present it, and see what it looks like
    - We assume that it will contain A, B, timestamp. 
    - Verify the same

2. Break it into 20 or so sets, by time.

3. Plot static graphs for these 20 sets.

4. Simulate attacks on these static graphs.
    - We want 3 graphs, for three different stages of the attack
    - Beginning of the attack, some point in the middle, and then when the critical fraction is reached.

5. Collect these three graphs and send them on forward to get to the next stage.

# New Section
"""

import pandas as pd
import numpy as np

# Read the math overflow data
# it should contain 3 columns, nodeA, nodeB, timestamp

df = pd.read_csv("sx-mathoverflow-a2q.txt", sep=" ", names=["A", "B", "timestamp"])
df = df.sort_values(by=['timestamp'])

## DIVIDE THE DATA INTO 20 GROUPS BASED ON TIMESTAMPS

# calculate the partition size
num_rows = len(df)
number_of_paritions = 5
partition_size = num_rows // number_of_paritions

# divide the data into partitions
partitions = []
for i in range(1, number_of_paritions+1):
    end_index = i*partition_size
    partition = df.iloc[:end_index]
    partitions.append(partition)

## WE HAVE THE 20 PARTITIONS
# Now, we make 20 static graphs, simulate attack on them.

import networkx as nx

graphs = []
for p in partitions:
    G = nx.from_pandas_edgelist(p, source='A', target='B', edge_attr='timestamp')
    graphs.append(G)

# here we have our graphs

import matplotlib.pyplot as plt
import random


def simulate_attack(graph):
    attack_fraction = 0.02
    attack_so_far = 0

    number_of_attacks_so_far = 0

    captured_graphs = []

    while attack_so_far < 0.4:
        # plot the appropriate data points for the current graph
        # capture_graph_data(graph)

        # remove attack_fraction
        def attack():
            nodes_by_degree = sorted(graph.nodes(), key=lambda x: graph.degree[x], reverse=True)
            num_nodes_to_remove = int(attack_fraction * graph.number_of_nodes())
            nodes_to_remove = nodes_by_degree[:num_nodes_to_remove]
            graph.remove_nodes_from(nodes_to_remove)


        def failure():
            # do nothing
            num_nodes_to_remove = int(attack_fraction * graph.number_of_nodes())
            nodes_remove = np.random.choice(graph.nodes(), size = num_nodes_to_remove, replace = False)
            graph.remove_nodes_from(nodes_remove)
        
        # attack()
        failure()

        captured_graphs.append(graph.copy())
        attack_so_far += attack_fraction
        number_of_attacks_so_far += 1
        

    return captured_graphs

import multiprocessing

result_graphs = []

pool = multiprocessing.Pool()
pool = multiprocessing.Pool(processes=20)
inputs = graphs

result_graphs = pool.map(simulate_attack, inputs)

print("Done")


import os


def store_graph_into_file(marked_graph):
    (graph_set_id, graph_id, graph) = marked_graph
    folder_name = "set" + str(graph_set_id)
    file_name = "graph" + str(graph_id) + ".txt"

    file_path = 'dpcn_result_graphs_failure/' + folder_name + '/' + file_name

    with open(file_path, 'w') as f:
        for edge in graph.edges(data=True):
            nodeA, nodeB, timestamp = edge[0], edge[1], edge[2]['timestamp']
            print(nodeA, nodeB, timestamp)
            f.write(f"{nodeA} {nodeB} {timestamp}\n")

    return 1

def store_graph_set_into_file(marked_graph_set):
    (i, graph_set) = marked_graph_set

    folder_name = "set" + str(i)

    dir_path = 'dpcn_result_graphs_failure/' + folder_name
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)

    input = [(i, j, graph) for (j, graph) in zip([i for i in range(len(graph_set))], graph_set)]

    for marked_graph in input:
        store_graph_into_file(marked_graph)


pool = multiprocessing.Pool()
pool = multiprocessing.Pool(processes=20)
input = zip([i for i in range(len(result_graphs))], result_graphs)

result_graphs = pool.map(store_graph_set_into_file, input)

print("Done")
